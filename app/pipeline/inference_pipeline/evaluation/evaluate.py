from __future__ import annotations

import argparse
import json
from pathlib import Path
from statistics import fmean

from app.core.logger_utils import get_logger
from app.pipeline.inference_pipeline.reasoning import ReasoningPipeline

from .metrics import (
    EvaluationMetric,
    HallucinationMetric,
    LevenshteinRatioMetric,
    ModerationMetric,
    ScoreResult,
)
from .style import Style

logger = get_logger(__name__)
DEFAULT_COLLECTIONS = ["dir_default"]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Evaluate the inference pipeline without relying on Opik.")
    parser.add_argument(
        "--dataset_name",
        type=str,
        default="LLMTwinMonitoringDataset",
        help="Logical name for the dataset (used to locate generated_dataset/* files).",
    )
    parser.add_argument(
        "--dataset_path",
        type=Path,
        default=None,
        help="Path to a JSON dataset. Overrides --dataset_name when provided.",
    )
    parser.add_argument(
        "--enable_rag",
        action="store_true",
        help="Evaluate with RAG enabled.",
    )
    parser.add_argument(
        "--collections",
        nargs="*",
        default=None,
        help="Optional list of Qdrant collection names used when RAG is enabled.",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Limit the number of samples evaluated.",
    )
    parser.add_argument(
        "--skip_style",
        action="store_true",
        help="Skip the style metric to avoid extra LLM judge calls.",
    )
    return parser.parse_args()


def infer_dataset_path(dataset_name: str) -> Path | None:
    dataset_dir = Path("generated_dataset")
    if not dataset_dir.exists():
        return None

    matches = sorted(dataset_dir.glob(f"*{dataset_name}*.json"))
    return matches[0] if matches else None


def resolve_dataset_path(dataset_name: str, dataset_path: Path | None) -> Path:
    resolved_candidates: list[Path] = []

    if dataset_path is not None:
        resolved_candidates.append(dataset_path)

    dataset_name_path = Path(dataset_name)
    if dataset_name_path.exists():
        resolved_candidates.append(dataset_name_path)

    inferred = infer_dataset_path(dataset_name)
    if inferred is not None:
        resolved_candidates.append(inferred)

    for candidate in resolved_candidates:
        if candidate.exists():
            return candidate

    raise FileNotFoundError(
        f"Could not find dataset for name '{dataset_name}'. Provide --dataset_path pointing to a JSON file generated by the dataset pipeline."
    )


def load_dataset(dataset_path: Path) -> list[dict[str, str]]:
    with dataset_path.open("r", encoding="utf-8") as source:
        raw_data = json.load(source)

    if not isinstance(raw_data, list):
        raise ValueError("Dataset file must contain a list of instruction/content objects.")

    dataset: list[dict[str, str]] = []
    for idx, entry in enumerate(raw_data):
        if not isinstance(entry, dict):
            logger.warning("Skipping entry %s because it is not an object.", idx)
            continue
        instruction = entry.get("instruction")
        reference = entry.get("content")
        if not instruction or not reference:
            logger.warning("Skipping entry %s because it lacks instruction/content.", idx)
            continue
        dataset.append({"instruction": instruction, "content": reference})

    if not dataset:
        raise ValueError("No valid samples found in dataset file.")

    return dataset


def evaluation_task(
    pipeline: ReasoningPipeline,
    record: dict[str, str],
    enable_rag: bool,
    doc_names: list[str],
) -> dict[str, str]:
    result = pipeline.generate(
        query=record["instruction"],
        enable_rag=enable_rag,
        doc_names=doc_names,
    )
    answer = result.get("answer") or ""
    context = result.get("context")

    if isinstance(context, list):
        context_text = "\n".join(str(item) for item in context)
    elif isinstance(context, str):
        context_text = context
    elif context is None:
        context_text = ""
    else:
        context_text = str(context)

    return {
        "input": record["instruction"],
        "output": answer,
        "reference": record["content"],
        "context": context_text,
    }


def safe_score(metric: EvaluationMetric, payload: dict[str, str]) -> ScoreResult:
    try:
        return metric.score(
            instruction=payload["input"],
            output=payload["output"],
            reference=payload["reference"],
            context=payload["context"],
        )
    except Exception as exc:  # pragma: no cover - defensive branch
        logger.exception("Metric %s failed: %s", metric.name, exc)
        return ScoreResult(
            name=metric.name,
            value=0.0,
            reason=f"Metric execution failed: {exc}",
        )


def run_evaluation(
    dataset: list[dict[str, str]],
    metrics: list[EvaluationMetric],
    enable_rag: bool,
    doc_names: list[str],
) -> dict[str, float]:
    pipeline = ReasoningPipeline(mock=False)
    aggregates = {metric.name: [] for metric in metrics}

    for index, record in enumerate(dataset, start=1):
        logger.info("Evaluating sample %s/%s", index, len(dataset))
        payload = evaluation_task(pipeline, record, enable_rag, doc_names)

        for metric in metrics:
            result = safe_score(metric, payload)
            aggregates[metric.name].append(result.value)
            logger.debug(
                "Metric %s => %.3f (%s)",
                metric.name,
                result.value,
                result.reason,
            )

    return {metric_name: fmean(values) if values else 0.0 for metric_name, values in aggregates.items()}


def main() -> None:
    args = parse_args()
    dataset_path = resolve_dataset_path(args.dataset_name, args.dataset_path)
    dataset = load_dataset(dataset_path)

    if args.limit is not None:
        dataset = dataset[: args.limit]

    doc_names = args.collections or DEFAULT_COLLECTIONS

    metrics: list[EvaluationMetric] = [
        LevenshteinRatioMetric(),
        HallucinationMetric(),
        ModerationMetric(),
    ]
    if not args.skip_style:
        metrics.append(Style())

    logger.info("Evaluating dataset '%s' from %s", args.dataset_name, dataset_path)
    if args.enable_rag:
        logger.info("RAG mode enabled using collections: %s", ", ".join(doc_names))

    summary = run_evaluation(dataset, metrics, args.enable_rag, doc_names)

    logger.info("Evaluation complete.")
    for metric_name, value in summary.items():
        logger.info("%s: %.3f", metric_name, value)


if __name__ == "__main__":
    main()
